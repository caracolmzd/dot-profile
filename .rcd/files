#!/bin/bash

# # #
# Files
#

# "print" to screen
alias p='cat'

# quick find
f() {
	find . -name "$*"
}

# quick search
s() {
	grep -Rni --exclude-dir .git --exclude-dir dist --exclude-dir build --exclude-dir vendor "$*"
}

#
# Listing Files 
#

#
# ## Cheat-sheet
# -C on columns
# -x on rows
# -1 [numeral] one column
# -F "Classify" indicate types (dir, link...)
# -A "almost all"
#
# ## Long Listings
# -i inode
# -o Owner and NOT Group
# -g Group and NOT owner
# --author
# -h human readable sizes
# -c modfied time
# -u accessed time
#
# ## Sort
# -r reverse
# -t time
# -S size
# --group-directories-first

alias ls='ls -F '${COLOR_AUTO}
# alias ls='ls -CF '${COLOR_AUTO}

alias l='ls -hl --group-directories-first'
alias ll='l -A' # (almost) all
alias la='ls -ltu' # sort by accessed
alias lt='ls -ltc' # sort by time (modified)
alias lS='ls -lS' # sort by size

alias lls='tree -L 2'
alias llls='tree -L 3'
alias lllls='tree -L 4'
alias ld='tree -dL 2'

# # #
# .git dirs are pruned, -o (or) files are printed
files-here() {	
	find . -path '*/.git' -prune -o -type f
}


find-hard-links() {
    directory=${1:-.}  # Set directory parameter to current directory if not provided
    depth=${2:-1}   # find -maxdepth, default 1
    shopt -s dotglob  # Include dot-files in the file globbing
    find "$directory" -maxdepth $depth -type f -links +1 -printf "%i %p\n" 2>/dev/null | sort -n
    shopt -u dotglob  # Reset dot-files globbing behavior
}

# # #
# use the -samefile feature of `find` to
# find alternate paths of the inode
same-file() {
    if [ $# -lt 1 ]; then
        echo "Usage: same-file <file_path> [starting_directory=.]"
        return 1
    fi

    file_path=$1
    starting_directory=${2:-.}  # Set starting directory to current directory if not provided

    echo -n "Searching for same-file..."

    find_output=$(find -L "$starting_directory" -samefile "$file_path" 2>/dev/null)

    echo "" >&2  # Move to a new line on stderr
    echo "$find_output"
}

# # #
# find all files in a given directory that are hard links
# and locate their other paths
find-all-links() {
    directory=${1:-.}  # Set directory parameter to current directory if not provided
    hard_links=$(find-hard-links "$directory")
    
    while read -r line; do
        inode=$(echo "$line" | awk '{print $1}')
        file=$(echo "$line" | awk '{print $2}')
        
        echo "inode searched: $inode"
        echo "Linked paths:"
        same-file "$file" "$directory" | grep -v "$file"
        echo "----"
    done <<< "$hard_links"
}


# # #
# get the most recently modified file
last-modified() {
  fullpath=$(find "$1" -maxdepth 1 -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d" ")
  basename "$fullpath"
}

# # #
# Consolidate duplicate files
dedupe() {
	args=${*:-.}
	rdfind -makehardlinks true -outputname dedupe.rpt $args
}

# # # 
# Web Archive
#

warc() {
	wget \
    --mirror \
    --page-requisites \
    --html-extension \
    --convert-links \
    --execute robots=off \
    --random-wait \
    "$1"
}
    # --directory-prefix=. \
    # --span-hosts \
    # --warc-file=w.arc \
    # --warc-cdx \
    # --domains=example.com,www.example.com,cdn.example.com \
    # --user-agent=Mozilla (mailto:archiver@petekeen.net)\
 
# Option List, wget
#     --mirror turns on a bunch of options appropriate for mirroring a whole website
#     --warc-file turns on WARC output to the specified file
#     --warc-cdx tells wget to dump out an index file for our new WARC file
#     --page-requisites will grab all of the linked resources necessary to render the page (images, css, javascript, etc)
#     --html-extension appends .html to the files when appropriate
#     --convert-links will turn links into local links as appropriate
#     --execute robots=off turns off wget's automatic robots.txt checking
#     --span-hosts allows it to follow links to other domain names
#     --domains includes a comma-separated list of domains that wget should include in the archive
#     --user-agent overrides wget's default user agent
#     --wait tells wget to wait ten seconds between each request
#     --random-wait will randomize that wait to between 5 and 15 seconds
#     http://www.example.com is the website we want to archive
